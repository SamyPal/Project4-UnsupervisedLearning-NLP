{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cc658dae75d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnews_website\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://transcriptdaily.com/?s=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Accept'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'text/html,application/xhtml+xml,*/*'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"user-agent\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_website\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urllib2' is not defined"
     ]
    }
   ],
   "source": [
    "#Modules needed\n",
    "import re\n",
    "#import urllib2\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "#print \"Libraries imported\"\n",
    "#Lists needed\n",
    "values=[]\n",
    "stocks=['AAPL']\n",
    "\n",
    "#Stocks list creation\n",
    "def extracting_values(file):\n",
    "    values=file[\"stock\"].values\n",
    "    stocks= values.tolist()\n",
    "    #del stocks[-4:]\n",
    "    print(\"\\nThe following stocks have been extracted and added to the stocks[] list: \" + str(stocks))\n",
    "#Load and read excel file\n",
    "#file = pandas.read_excel('20160706 - missing earnings - Copy.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "#Going to news website for each stock.\n",
    "for stock in stocks:\n",
    "    news_found=0\n",
    "    news_website=\"https://transcriptdaily.com/?s=\" + str(stock)\n",
    "    hdr = {'Accept': 'text/html,application/xhtml+xml,*/*',\"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36\"}\n",
    "    req=urllib2.Request(news_website,headers=hdr)\n",
    "    html = urllib2.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    print(\"\\nLooking for news related to the target company: \" + (str(stock)))\n",
    "#Getting all news links    \n",
    "    for link in soup.find_all('a', href=re.compile(str(stock).lower())):\n",
    "        exact_news=[]\n",
    "        exact_news.append(link['href'])\n",
    "        if news_found==0:\n",
    "            pass\n",
    "        elif news_found==1:\n",
    "            break\n",
    "#Getting text from target news articles        \n",
    "        for news in exact_news:\n",
    "            news_text=0\n",
    "            news_text=news\n",
    "            req_news=urllib2.Request(news_text,headers=hdr)\n",
    "            html_news = urllib2.urlopen(req_news).read()\n",
    "            target_news_soup=BeautifulSoup(html_news)\n",
    "            for text in target_news_soup.find_all('p', text=re.compile(\"last issued its earnings results\")):\n",
    "                print(\"\\nHere are the actual news lines for: \" + str(stock))\n",
    "                print(text)\n",
    "                news_found=1\n",
    "            for text in target_news_soup.find_all('p', text=re.compile(\"last issued its earnings data\")):\n",
    "                print(\"\\nHere are the actual news lines for: \" + str(stock))\n",
    "                print(text)\n",
    "                news_found=1\n",
    "print(\"\\nAll done!\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# =======\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Created on Wed Jul 19 18:15:11 2017\n",
    "# @author: simeon\n",
    "# WEB SCRAPER for stocks info. Tradespoon real world project. \n",
    "# \"\"\"\n",
    "# #Modules needed\n",
    "# import re\n",
    "# import urllib2\n",
    "# import pandas\n",
    "# from bs4 import BeautifulSoup\n",
    "# print \"Libraries imported\"\n",
    "# #Lists needed\n",
    "# values=[]\n",
    "# stocks=[]\n",
    "\n",
    "\n",
    "# #Load and read excel file\n",
    "# file = pandas.read_excel('20160706 - missing earnings - Copy.xlsx')\n",
    "\n",
    "# #Stocks list creation\n",
    "# def extracting_values(file):\n",
    "#     values=file[\"stock\"].values\n",
    "#     stocks= values.tolist()\n",
    "#     del stocks[-4:]\n",
    "#     print \"\\nThe following stocks have been extracted and added to the stocks[] list: \" + str(stocks)\n",
    "\n",
    "# #Going to news website for each stock.\n",
    "# for stock in stocks:\n",
    "#     news_found=0\n",
    "#     news_website=\"https://transcriptdaily.com/?s=\" + str(stock)\n",
    "#     hdr = {'Accept': 'text/html,application/xhtml+xml,*/*',\"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36\"}\n",
    "#     req=urllib2.Request(news_website,headers=hdr)\n",
    "#     html = urllib2.urlopen(req).read()\n",
    "#     soup = BeautifulSoup(html)\n",
    "#     print\"\\nLooking for news related to the target company: \" + (str(stock))\n",
    "# #Getting all news links    \n",
    "#     for link in soup.find_all('a', href=re.compile(str(stock).lower())):\n",
    "#         exact_news=[]\n",
    "#         exact_news.append(link['href'])\n",
    "#         if news_found==0:\n",
    "#             pass\n",
    "#         elif news_found==1:\n",
    "#             break\n",
    "# #Getting text from target news articles        \n",
    "#         for news in exact_news:\n",
    "#             news_text=0\n",
    "#             news_text=news\n",
    "#             req_news=urllib2.Request(news_text,headers=hdr)\n",
    "#             html_news = urllib2.urlopen(req_news).read()\n",
    "#             target_news_soup=BeautifulSoup(html_news)\n",
    "#             for text in target_news_soup.find_all('p', text=re.compile(\"last issued its earnings results\")):\n",
    "#                 print \"\\nHere are the actual news lines for: \" + str(stock)\n",
    "#                 print text\n",
    "#                 news_found=1\n",
    "#             for text in target_news_soup.find_all('p', text=re.compile(\"last issued its earnings data\")):\n",
    "#                 print \"\\nHere are the actual news lines for: \" + str(stock)\n",
    "#                 print text\n",
    "#                 news_found=1\n",
    "# print (\"\\nAll done!\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
